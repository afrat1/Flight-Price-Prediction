{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995bfd22-f660-4a28-be20-24bc8f899f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec7c815-c4fd-4028-bcd2-3275412e637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_path, sample_ratio=0.01, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Loads, preprocesses and splits data into train-test sets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path: str\n",
    "        Path to CSV file\n",
    "    sample_ratio: float\n",
    "        Ratio of data to use (between 0-1)\n",
    "    train_ratio: float\n",
    "        Training set ratio (between 0-1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, X_test, y_train, y_test: numpy arrays\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    df = df.drop(['Unnamed: 0', 'flight'], axis=1)\n",
    "    \n",
    "    # Encode categorical data\n",
    "    df['class'] = df['class'].apply(lambda x: 1 if x == 'Business' else 0)\n",
    "    df.stops = pd.factorize(df.stops)[0]\n",
    "    \n",
    "    # One-hot encoding uygula\n",
    "    categorical_columns = ['airline', 'source_city', 'destination_city', \n",
    "                         'arrival_time', 'departure_time']\n",
    "    for col in categorical_columns:\n",
    "        df = df.join(pd.get_dummies(df[col], prefix=col)).drop(col, axis=1)\n",
    "    \n",
    "    # Features ve target'ı ayır\n",
    "    X, y = df.drop('price', axis=1).values, df.price.values\n",
    "    \n",
    "    # Veri setini örnekle\n",
    "    if sample_ratio < 1.0:\n",
    "        random_indices = np.random.permutation(len(X))\n",
    "        subset_size = int(len(X) * sample_ratio)\n",
    "        selected_indices = random_indices[:subset_size]\n",
    "        X = X[selected_indices]\n",
    "        y = y[selected_indices]\n",
    "    \n",
    "    # Train-test split\n",
    "    split_index = int(len(X) * train_ratio)\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b35bc2-ac4e-461b-81f1-05120273335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    \"\"\"\n",
    "    Custom Decision Tree Regressor implementation\n",
    "    Used as base learner in the Gradient Boosting model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=3, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1]\n",
    "        self.tree = self._grow_tree(X, y, depth=0)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth):\n",
    "        \"\"\"\n",
    "        Recursively grows the decision tree\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: array-like\n",
    "            Training data\n",
    "        y: array-like\n",
    "            Target values\n",
    "        depth: int\n",
    "            Current depth in the tree\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict or float\n",
    "            Either a decision node (dict) or leaf value (float)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Check stopping criteria\n",
    "        if (depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or \n",
    "            np.std(y) < 1e-6):  # Nearly pure node\n",
    "            return self._create_leaf(y)\n",
    "\n",
    "        # Find the best split\n",
    "        best_feature, best_threshold = self._find_best_split(X, y)\n",
    "        \n",
    "        if best_feature is None:  # No valid split found\n",
    "            return self._create_leaf(y)\n",
    "            \n",
    "        # Split data based on best feature and threshold\n",
    "        left_idxs = X[:, best_feature] <= best_threshold\n",
    "        right_idxs = ~left_idxs\n",
    "        \n",
    "        # Recursively build left and right subtrees\n",
    "        left_tree = self._grow_tree(X[left_idxs], y[left_idxs], depth + 1)\n",
    "        right_tree = self._grow_tree(X[right_idxs], y[right_idxs], depth + 1)\n",
    "        \n",
    "        return {'feature': best_feature,\n",
    "                'threshold': best_threshold,\n",
    "                'left': left_tree,\n",
    "                'right': right_tree}\n",
    "\n",
    "    def _create_leaf(self, y):\n",
    "        return np.mean(y)\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_gain = -np.inf\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature in range(self.n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                gain = self._calculate_variance_reduction(X[:, feature], y, threshold)\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                    \n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _calculate_variance_reduction(self, X_column, y, threshold):\n",
    "        \"\"\"\n",
    "        Calculate variance reduction for a potential split\n",
    "        This is the splitting criterion used for regression trees\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            The reduction in variance achieved by this split\n",
    "        \"\"\"\n",
    "        # Calculate parent node variance\n",
    "        parent_var = np.var(y) * len(y)\n",
    "        \n",
    "        # Split data\n",
    "        left_idxs = X_column <= threshold\n",
    "        right_idxs = ~left_idxs\n",
    "        \n",
    "        # Check if split is valid\n",
    "        if np.sum(left_idxs) == 0 or np.sum(right_idxs) == 0:\n",
    "            return -np.inf\n",
    "        \n",
    "        # Calculate variance for children\n",
    "        left_var = np.var(y[left_idxs]) * np.sum(left_idxs)\n",
    "        right_var = np.var(y[right_idxs]) * np.sum(right_idxs)\n",
    "        \n",
    "        # Calculate variance reduction\n",
    "        variance_reduction = parent_var - (left_var + right_var)\n",
    "        return variance_reduction\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if not isinstance(node, dict):\n",
    "            return node\n",
    "            \n",
    "        if x[node['feature']] <= node['threshold']:\n",
    "            return self._traverse_tree(x, node['left'])\n",
    "        return self._traverse_tree(x, node['right'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c111765-35ee-4742-8263-ec89c73f4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGBR:\n",
    "    \"\"\"\n",
    "    Simple Gradient Boosting Regressor implementation\n",
    "    Similar to XGBoost but with basic functionality\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        \"\"\"\n",
    "        Initialize the Gradient Boosting Regressor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_estimators: int\n",
    "            Number of boosting stages (trees) to perform\n",
    "        learning_rate: float\n",
    "            Step size shrinkage used to prevent overfitting\n",
    "        max_depth: int\n",
    "            Maximum depth of individual regression trees\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.base_pred = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the gradient boosting model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: array-like\n",
    "            Training data features\n",
    "        y: array-like\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        # Initialize prediction with mean of target values\n",
    "        self.base_pred = np.mean(y)\n",
    "        residual = y - self.base_pred\n",
    "\n",
    "        # Iteratively train trees on residuals\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residual)\n",
    "            pred = tree.predict(X)\n",
    "            self.models.append(tree)\n",
    "            # Update residuals based on current prediction\n",
    "            residual -= self.learning_rate * pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(X.shape[0], self.base_pred)\n",
    "        for tree in self.models:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbbf9ef2-f252-4af9-9fd7-9322c9acbaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Visualizes model results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_test: numpy array\n",
    "        Actual values\n",
    "    y_pred: numpy array\n",
    "        Predicted values\n",
    "    \"\"\"\n",
    "    # Calculate residuals\n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    # Main visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 1. Predicted vs Actual Values Scatter Plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    density = plt.hist2d(y_test, y_pred, bins=50, cmap='viridis',\n",
    "                        norm=plt.matplotlib.colors.LogNorm())\n",
    "    plt.colorbar(density[3], label='Number of points')\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Predicted Price')\n",
    "    plt.title('Prediction vs Actual Values\\nColor density shows number of points')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 2. Residuals Plot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Price')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals vs Predictions')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 3. Residuals Distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.hist(residuals, bins=50, edgecolor='black')\n",
    "    plt.xlabel('Residual')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Residuals')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 4. Error Distribution Box Plot\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.boxplot(residuals)\n",
    "    plt.ylabel('Prediction Error')\n",
    "    plt.title('Distribution of Prediction Errors')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Error distribution by price range\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    price_ranges = pd.qcut(y_test, q=10)\n",
    "    mean_errors = pd.DataFrame({'residuals': abs(residuals)}).groupby(price_ranges, observed=True).mean()\n",
    "    plt.bar(range(len(mean_errors)), mean_errors['residuals'])\n",
    "    plt.xlabel('Price Range (Deciles)')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.title('Prediction Error by Price Range')\n",
    "    plt.xticks(range(len(mean_errors)), \n",
    "              ['Low', '2', '3', '4', '5', '6', '7', '8', '9', 'High'], \n",
    "              rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a240ff7-eac7-48e4-ba62-1320ec8cfce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "file_path = './dataset/Clean_Dataset_.csv'\n",
    "X_train, X_test, y_train, y_test = preprocess_data(\n",
    "    file_path, \n",
    "    sample_ratio=0.1,  # Use full dataset\n",
    "    train_ratio=0.8  # 80% for training, 20% for testing\n",
    ")\n",
    "\n",
    "# Initialize and train the model\n",
    "model = SimpleGBR(\n",
    "    n_estimators=120,    # Number of trees\n",
    "    learning_rate=0.1,   # Learning rate for gradient descent\n",
    "    max_depth=3         # Maximum depth of each tree\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and calculate metrics\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "r2 = 1 - (sum((y_test - y_pred) ** 2) / sum((y_test - np.mean(y_test)) ** 2))\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "mse = np.mean((y_test - y_pred) ** 2)\n",
    "rmse = math.sqrt(mse)\n",
    "\n",
    "print(\"R2:\", r2)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "visualize_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9544eaa-52b1-43ce-a3d2-3f34d334194c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
